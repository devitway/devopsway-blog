---
title: "Qwen Code — бесплатная альтернатива Claude Code для DevOps-инженеров"
date: 2026-02-25
draft: false
description: "Честный обзор Qwen Code v0.10.6: облачный и локальный режимы, реальные тесты на Ubuntu-десктопе с RTX 3090, грабли с Ollama и матрица выбора для DevOps-задач"
categories: ["AI", "DevOps", "Tutorial"]
tags: ["qwen-code", "ollama", "devops", "cli", "open-source"]
---

Claude Code — отличный инструмент, но $20/мес за Pro (а реально нужен Max за $100+) — это бюджет не для каждой команды. Мы нашли бесплатную альтернативу, которая реально работает на DevOps-задачах: **Qwen Code**.

Ниже — честный отчёт после недели использования v0.10.6 на рабочем десктопе с Ubuntu (Intel i3-8100, 16 GB RAM, RTX 3090). С граблями, хаками и финальной матрицей выбора.

## Часть 1: Облачный режим — 3 минуты до первого результата

### Установка

```bash
npm install -g @qwen-code/qwen-code@latest
```

Запускаем, вводим `/auth` — открывается Qwen OAuth в браузере. Три секунды, ноль конфигурации. На бесплатном тарифе — 1000 запросов в день, чего для DevOps-рутины хватает с запасом.

### Что тестировали (реальные команды)

Все тесты — на рабочей машине с реальными сервисами, не в песочнице.

**Базовая диагностика — без сюрпризов:**

```
> docker --version
Docker version 29.2.0, build c5f937a

> df -h
```

Qwen Code выдал аккуратную ASCII-таблицу с дисками и добавил итоговую строку: `/ (sda1): 181G/225G (86%), D1: 695G/932G (75%)`. Claude Code делает то же самое, но тут это бесплатно.

**Топ-5 процессов по RAM — self-correction в действии:**

```
> покажи топ-5 процессов по RAM
```

Первая попытка — `ps aux --sort=-%mem | head -6` — вывод получился грязный, с обрезанными колонками. Qwen Code *сам* заметил это, переделал команду с кастомным форматированием `ps -eo pid,user,%mem,%cpu,comm --sort=-%mem | head -6` и вернул чистую таблицу. Никаких доп. промптов от меня — self-correction сработал автоматически.

**Сетевые порты:**

```
> ss -tlnp
```

Разбил на две команды — `ss -tlnp` для TCP и `ss -ulnp` для UDP — и выдал итоговые таблицы с именами сервисов. Ничего лишнего.

**Bash-скрипт мониторинга — обход ограничений:**

```
> напиши bash-скрипт мониторинга CPU/RAM/диска с алертами
```

Тут произошло интересное. Инструмент `WriteFile` отказал: `File path must be within workspace`. Qwen Code не завис и не выдал ошибку пользователю, а переключился на `cat > /tmp/sysinfo.sh << 'EOF'` через bash-команду. Скрипт записался, `chmod +x` получил, запустился — вернул валидный JSON с CPU, RAM, uptime. Хитрый.

### Что впечатлило

Три вещи, которые отличают Qwen Code от «просто обёртки над API»:

**Self-correction.** Грязный вывод команды → автоматическая переделка с лучшим форматированием. Без моего вмешательства.

**Обход ограничений.** WriteFile не работает → `cat >` через bash. Инструмент не падает, а ищет альтернативный путь.

**Форматирование.** ASCII-таблицы с выравниванием, итоговые строки, группировка. Мелочь, но при ежедневном использовании экономит время на парсинг вывода глазами.

### Ограничения облачного режима

Не всё гладко.

**Язык.** Пишешь вопрос на русском — получаешь ответ на английском. Команды выполняет корректно, но пояснения — English only. Для DevOps это терпимо, но раздражает.

**Модель-инкогнито.** Qwen Code не знает, какая модель работает под капотом — спрашиваешь, получаешь «I'm a coder-model». Ни версии, ни параметров.

**Encoding-баг.** При копировании кириллицы из терминала иногда ломается кодировка. Воспроизводится нестабильно, но если работаете с русскоязычными конфигами — имейте в виду.

### Бесплатные провайдеры, которые НЕ работают

Мы пробовали подключить сторонние бесплатные API — сэкономить ещё больше. Спойлер: не вышло.

**Groq free tier** — системный промпт Qwen Code + контекст весит ~11.5K токенов, а Groq на бесплатном тарифе даёт лимит TPM в 8000. Итог — `413 Request too large, Limit 8000, Requested 11566` на каждый запрос. При этом модель определяется как `openai/gpt-oss-20b` — не факт, что это вообще Qwen под капотом. Мёртвый вариант.

**OpenRouter qwen3-coder:free** — лимит 8 запросов в минуту, и при активной работе (tool calling генерирует цепочки запросов) получаем `Request cancelled`. Для интерактивного использования непригодно.

Вывод: облачный Qwen OAuth — единственный рабочий бесплатный вариант на данный момент.

---

## Часть 2: Локальный режим (Ollama) — приватность, без лимитов, оффлайн

Облако — удобно, но не всегда подходит. NDA на инфраструктуру, air-gapped сети, или просто принципиальная позиция «данные не покидают периметр». Поднимаем локально.

### Железо и окружение

Та же рабочая машина: Intel i3-8100, 16 GB RAM, **RTX 3090** (24 GB VRAM). Ollama крутится локально на `192.168.1.23:11434`. Всё, что описано ниже, воспроизводимо на любой карте с 16+ GB VRAM для 14b-моделей.

### settings.json — подключаем Qwen Code к Ollama

```json
{
  "provider": "openai",
  "openaiKey": "ollama",
  "openaiBaseUrl": "http://192.168.1.23:11434/v1"
}
```

Да, `"openaiKey": "ollama"` — это не шутка, а реальный конфиг. Ollama эмулирует OpenAI-совместимый API, Qwen Code подключается к нему как к обычному провайдеру. Файл кладём в `~/.qwen-code/settings.json`.

### Модели, которые РАБОТАЮТ

| Модель | Tool calling | Thinking | Скорость | Комментарий |
|--------|:-----------:|:--------:|----------|-------------|
| `qwen3:14b` | ✅ | ✅ | ~8 сек | Основной рабочий вариант |
| `qwen3:14b-nothink` | ✅ | ❌ | ~4 сек | Наш кастомный, без «думания вслух» |
| `qwen3:30b-a3b` | ✅ | ✅ | ~12 сек | Тяжелее но умнее |
| `llama3.2:3b` | ✅ | ❌ | ~1.4 сек | Быстрый, для простых задач |

**`qwen3:14b`** — золотая середина. Tool calling работает из коробки, модель «думает» перед ответом (chain-of-thought), качество на DevOps-задачах вполне достаточное. Если нужен один вариант — берите этот.

**`llama3.2:3b`** — 1.4 секунды на ответ. Для команд вида «покажи версию Docker» или «какой IP у eth0» — идеально. Для сложных скриптов — не хватает мозгов, но как быстрый помощник на каждый день — отлично.

### Модели, которые НЕ РАБОТАЮТ

Тут грабли, на которые мы наступили — чтобы вам не пришлось.

**`qwen3-coder:30b` — ЛОВУШКА.** Название кричит «я создана для кодинга». Реальность — у неё **нет tool calling**. Qwen Code отправляет запрос на вызов инструмента, модель возвращает обычный текст. Результат — бесконечный цикл retry. Тридцать минут отладки, прежде чем разобрались.

**`phi4-mini`** — мусор в ответах. Вместо структурированного JSON для tool call возвращает обрывки XML, незакрытые теги, галлюцинации. Непригодна.

**`qwen2.5-coder:32b`** — отдаёт tool calls в **XML-формате**, а Qwen Code ожидает JSON. Напрямую через Ollama — не работает.

### Главная грабля: thinking mode в Ollama

`qwen3:14b` по умолчанию «думает вслух» — перед каждым ответом выдаёт блок `<think>...</think>` с цепочкой рассуждений. На простых командах это +3-4 секунды бессмысленного ожидания.

Логичное решение — отключить. Но **Ollama 0.15.2 не поддерживает `PARAMETER think false`** в Modelfile. Параметр просто игнорируется.

**Наш хак — кастомный Modelfile (упрощённая версия для 14b):**

```dockerfile
FROM qwen3:14b

TEMPLATE """{{- range .Messages }}
{{- if eq .Role "system" }}<|im_start|>system
{{ .Content }}<|im_end|>
{{- else if eq .Role "user" }}<|im_start|>user
{{ .Content }} /no_think<|im_end|>
{{- else if eq .Role "assistant" }}<|im_start|>assistant
{{ .Content }}<|im_end|>
{{- end }}
{{- end }}<|im_start|>assistant
"""
```

Трюк — `/no_think` добавляется в конец каждого user-сообщения прямо в шаблоне. Модель видит эту инструкцию и перестаёт генерировать блок `<think>`.

> **Примечание:** для `qwen3:30b-a3b` мы используем более сложный Modelfile с полным tool calling template, `<tool_call>` XML-тегами и динамическим управлением thinking через `$lastUserIdx`. Упрощённая версия выше покрывает 90% DevOps-задач на 14b.

```bash
ollama create qwen3:14b-nothink -f Modelfile
```

Результат: `qwen3:14b-nothink` — та же модель, те же tool calls, но без «размышлений». Ответ за ~4 секунды вместо ~8. На рутинных DevOps-задачах разницы в качестве мы не заметили.

### MCP-серверы

MCP (Model Context Protocol) работает так же, как в Claude Code — через секцию `mcpServers` в `settings.json`:

```json
{
  "mcpServers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/home/user/projects"]
    }
  }
}
```

Подключали файловую систему и GitHub — завелось без проблем. Но есть нюанс по скорости.

### Скорость tool calling через sub-agent

Каждый вызов инструмента в локальном режиме — это отдельный запрос к модели. Цепочка «думаю → вызываю tool → получаю результат → думаю → отвечаю» занимает **~40 секунд** на `qwen3:14b`.

Для интерактивной работы это ощутимо. Для фоновых задач (сгенерить скрипт, проанализировать лог-файл) — терпимо. На `llama3.2:3b` цепочка быстрее, но и качество tool calls хуже — модель чаще ошибается с аргументами.

---

## Матрица выбора

| Задача | Конфигурация | Время настройки |
|--------|-------------|:---------------:|
| Попробовать | Qwen OAuth (облако) | ~3 мин |
| Приватность | Ollama + `qwen3:14b-nothink` | ~15 мин |
| Макс. качество | Ollama + `qwen3:30b-a3b` | ~20 мин |
| Макс. скорость | Ollama + `llama3.2:3b` | ~10 мин |

**Попробовать** — `npm install`, `/auth`, работаете. Если нужно просто посмотреть, что такое AI-ассистент в терминале — начинайте с облака.

**Приватность** — данные не покидают вашу сеть. 15 минут на установку Ollama, скачивание модели и создание кастомного Modelfile без thinking. Рабочая лошадка для ежедневных DevOps-задач.

**Макс. качество** — `qwen3:30b-a3b` умнее 14b-модели, лучше справляется со сложными скриптами и анализом конфигов. Цена — больше VRAM и чуть дольше ответ.

**Макс. скорость** — `llama3.2:3b` отвечает за 1.4 секунды. Для быстрых справочных запросов и простых команд — быстрее, чем гуглить.

---

## Итог

Qwen Code — не замена Claude Code. У Claude лучше качество ответов, глубже понимание контекста, стабильнее работа с большими проектами. Но для DevOps-задач в терминале — проверить порты, написать скрипт мониторинга, разобраться с процессами — Qwen Code справляется.

Главное преимущество: это бесплатно и работает локально. Если ваши данные не могут покидать периметр, или бюджет на AI-инструменты равен нулю — Qwen Code закрывает базовые потребности.

Главный минус: грабли на каждом шагу. Модели-ловушки без tool calling, баг с thinking в Ollama, неработающие бесплатные провайдеры. Всё решаемо, но будьте готовы потратить вечер на настройку.

Рабочий минимум: `npm install -g @qwen-code/qwen-code@latest && qwen-code` → `/auth` → работаете.
