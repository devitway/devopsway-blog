---
title: "Claude Code CLI + RAG + Context Engineering: Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ³Ğ°Ğ¹Ğ´"
date: 2026-02-05T12:00:00+03:00
lastmod: 2026-02-05T12:00:00+03:00
draft: false
weight: 1
categories: ["AI", "DevOps", "Tutorial"]
tags: ["claude-code", "rag", "ollama", "qdrant", "mcp", "context-engineering", "devops", "llm", "vector-database", "litellm"]
author: "DevOps Way"
series: ""
description: "ĞšĞ°Ğº Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Claude Code CLI Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ RAG Ğ¸ Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ context engineering Ğ²Ğ°Ğ¶Ğ½ĞµĞµ prompt engineering. ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ ÑÑ‚ĞµĞº: Ollama, LiteLLM, Qdrant, MCP."
canonical: ""
showToc: true
TocOpen: true
hidemeta: false
comments: true
disableHLJS: false
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: true
ShowRssButtonInSectionTermList: true
UseHugoToc: true
cover:
    image: ""
    alt: "Claude Code + RAG + Context Engineering"
    caption: "Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ AI-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹"
    relative: false
    hidden: false
editPost:
    URL: ""
    Text: "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ÑŒ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ"
    appendFilePath: true

---

## TL;DR

**Prompt engineering â€” ÑÑ‚Ğ¾ 5% ÑƒÑĞ¿ĞµÑ…Ğ°. Context engineering â€” Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ 95%.**

ĞœÑ‹ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ğ¼ AI-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹:
- Ğ—Ğ½Ğ°ĞµÑ‚ Ğ²Ğ°ÑˆÑƒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ (759 Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² â†’ 16,548 Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²)
- Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ (tool calling)
- Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ (RTX 3090, Ğ½Ğ¸ĞºĞ°ĞºĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ²)
- ĞÑ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ Ğ·Ğ° 10-15 ÑĞµĞºÑƒĞ½Ğ´

---

## Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Claude Code CLI

```bash
# Ğ¢Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Node.js 18+
node --version  # v18.x Ğ¸Ğ»Ğ¸ Ğ²Ñ‹ÑˆĞµ

# Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ñ‡ĞµÑ€ĞµĞ· npm
npm install -g @anthropic-ai/claude-code

# Ğ˜Ğ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· npx (Ğ±ĞµĞ· ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸)
npx @anthropic-ai/claude-code

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ°
claude --version
```

**ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ Ğ·Ğ°Ğ¿ÑƒÑĞº:**

```bash
# Ğ—Ğ°Ğ¿ÑƒÑĞº (Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ API ĞºĞ»ÑÑ‡ Anthropic)
claude

# Ğ˜Ğ»Ğ¸ Ñ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
claude --model claude-sonnet-4-20250514
```

ĞŸÑ€Ğ¸ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ Ğ·Ğ°Ğ¿ÑƒÑĞºĞµ Claude Code Ğ¿Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¸Ñ‚ Ğ²Ğ²ĞµÑÑ‚Ğ¸ `ANTHROPIC_API_KEY`. ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ĞºĞ»ÑÑ‡: [console.anthropic.com](https://console.anthropic.com)

> ğŸ’¡ Ğ’ ÑÑ‚Ğ¾Ğ¼ Ğ³Ğ°Ğ¹Ğ´Ğµ Ğ¼Ñ‹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¸Ğ¼ **Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‡ĞµÑ€ĞµĞ· LiteLLM**, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Ğ¿Ğ»Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ·Ğ° API.

---

## Ğ§Ğ°ÑÑ‚ÑŒ 0: Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ Context Engineering

> *"The hottest new programming paradigm is English â€” but the context window is your real IDE"*
>
> *"Ğ¡Ğ°Ğ¼Ğ°Ñ Ğ³Ğ¾Ñ€ÑÑ‡Ğ°Ñ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ â€” ÑÑ‚Ğ¾ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº. ĞĞ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾ĞºĞ½Ğ¾ â€” Ğ²Ğ¾Ñ‚ Ñ‚Ğ²Ğ¾Ñ Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰Ğ°Ñ IDE"*
>
> â€” Andrej Karpathy

### Prompt vs Context

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PROMPT ENGINEERING                          â”‚
â”‚                                                                  â”‚
â”‚  "Ğ¢Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ñ‚ Ğ¿Ğ¾ nginx. ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹ ĞºÑ€Ğ°Ñ‚ĞºĞ¾. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹."      â”‚
â”‚                                                                  â”‚
â”‚  âŒ ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²ÑÑ‘ Ñ€Ğ°Ğ²Ğ½Ğ¾ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚                              â”‚
â”‚  âŒ ĞĞµ Ğ·Ğ½Ğ°ĞµÑ‚ Ğ’ĞĞ¨Ğ£ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ                                  â”‚
â”‚  âŒ ĞĞµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CONTEXT ENGINEERING                          â”‚
â”‚                                                                  â”‚
â”‚  System Prompt (Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ)                                       â”‚
â”‚       +                                                          â”‚
â”‚  RAG Results (Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ’ĞĞ¨Ğ•Ğ™ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸)                     â”‚
â”‚       +                                                          â”‚
â”‚  Tool Results (Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹, Ğ²Ñ‹Ğ²Ğ¾Ğ´ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´)                    â”‚
â”‚       +                                                          â”‚
â”‚  Conversation History (ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°)                        â”‚
â”‚       =                                                          â”‚
â”‚  ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚                                              â”‚
â”‚                                                                  â”‚
â”‚  âœ… ĞÑ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…                          â”‚
â”‚  âœ… Ğ¡ÑÑ‹Ğ»Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ¸ ÑÑ‚Ñ€Ğ¾ĞºĞ¸                      â”‚
â”‚  âœ… ĞœĞ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ĞĞ½Ğ°Ñ‚Ğ¾Ğ¼Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ½Ğ°ÑˆĞµĞ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONTEXT WINDOW (~16K tokens)                  â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 1. SYSTEM PROMPT (~500 tokens)                          â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚ "You are DevOps mentor. ALWAYS use tools first.         â”‚    â”‚
â”‚  â”‚  Use Socratic method. Speak Russian."                   â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚ â†’ ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ ĞŸĞĞ’Ğ•Ğ”Ğ•ĞĞ˜Ğ•                                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              +                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 2. RAG RESULTS (~2000 tokens)                           â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚ [search_knowledge("nginx reverse proxy")]               â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚ Result 1: nginx-guide.md:45-89 (score: 0.92)           â”‚    â”‚
â”‚  â”‚ "## Reverse Proxy                                       â”‚    â”‚
â”‚  â”‚  location /api/ { proxy_pass http://backend:8000; }"   â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚ Result 2: troubleshooting.md:12-34 (score: 0.85)       â”‚    â”‚
â”‚  â”‚ "### ĞÑˆĞ¸Ğ±ĞºĞ° 502 Bad Gateway..."                         â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚ â†’ ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ—ĞĞĞĞ˜Ğ¯                                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              +                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 3. TOOL RESULTS (~1000 tokens)                          â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚ [Read /etc/nginx/nginx.conf]                            â”‚    â”‚
â”‚  â”‚ "server { listen 80; location / { ... } }"             â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚ [Bash: nginx -t]                                        â”‚    â”‚
â”‚  â”‚ "nginx: configuration file syntax is ok"                â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚ â†’ ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¤ĞĞšĞ¢Ğ« (ground truth)                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              +                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 4. CONVERSATION HISTORY (~2000 tokens)                  â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚ User: "nginx Ğ½Ğµ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ÑÑ"                            â”‚    â”‚
â”‚  â”‚ Assistant: "Ğ”Ğ°Ğ²Ğ°Ğ¹ Ğ¿Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¸Ğ¼ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³..."                  â”‚    â”‚
â”‚  â”‚ User: "Ğ²Ğ¾Ñ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: ..."                                 â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚ â†’ ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ ĞšĞĞĞ¢Ğ•ĞšĞ¡Ğ¢ Ğ”Ğ˜ĞĞ›ĞĞ“Ğ                           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              =                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 5. LLM RESPONSE                                         â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚ "Ğ’Ğ¸Ğ¶Ñƒ Ğ² nginx.conf:45 Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ° Ñ‚Ğ¾Ñ‡ĞºĞ° Ñ Ğ·Ğ°Ğ¿ÑÑ‚Ğ¾Ğ¹.       â”‚    â”‚
â”‚  â”‚  Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ Ğ½Ğ°ÑˆĞµĞ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ (nginx-guide.md),         â”‚    â”‚
â”‚  â”‚  ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¸Ğ²Ğ° Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ·Ğ°ĞºĞ°Ğ½Ñ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° ';'          â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚  Ğ§Ñ‚Ğ¾ Ğ±ÑƒĞ´ĞµÑ‚ ĞµÑĞ»Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ ';' Ğ² ĞºĞ¾Ğ½ĞµÑ† ÑÑ‚Ñ€Ğ¾ĞºĞ¸ 45?"       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ğ“Ğ´Ğµ Ğ¼Ñ‹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚

| Ğ¡Ğ»Ğ¾Ğ¹ | Ğ§Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµĞ¼ | Ğ—Ğ°Ñ‡ĞµĞ¼ |
|------|------------|-------|
| **Chunking** | `CHUNK_SIZE=800`, overlap=150 | ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ "Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹" |
| **Metadata** | file_path, start_line, end_line | ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ¾ÑĞ»Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº |
| **Retrieval** | top-5 Ğ¿Ğ¾ cosine similarity | Ğ ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ½Ğµ ÑˆÑƒĞ¼ |
| **Score** | ĞŸĞµÑ€ĞµĞ´Ğ°Ñ‘Ğ¼ score Ğ² Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ñ… | ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´Ğ¸Ñ‚ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ |
| **System Prompt** | Tools first, Socratic method | ĞŸĞ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° |
| **Tool Design** | JSON output, truncation | ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·ÑƒĞµĞ¼Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ |

---

## Ğ§Ğ°ÑÑ‚ÑŒ 1: ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Ğ’Ğ°Ñˆ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ°Ğ»                             â”‚
â”‚                             â”‚                                   â”‚
â”‚                    claude "Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ"                              â”‚
â”‚                             â”‚                                   â”‚
â”‚                             â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                  Claude Code CLI                         â”‚   â”‚
â”‚  â”‚                         â”‚                                â”‚   â”‚
â”‚  â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚   â”‚
â”‚  â”‚         â–¼               â–¼               â–¼               â”‚   â”‚
â”‚  â”‚    LiteLLM         MCP Server       Built-in            â”‚   â”‚
â”‚  â”‚    :4000            :4002           tools               â”‚   â”‚
â”‚  â”‚      â”‚                â”‚              (Bash,Read,...)    â”‚   â”‚
â”‚  â”‚      â–¼                â–¼                                 â”‚   â”‚
â”‚  â”‚   Ollama           Qdrant                               â”‚   â”‚
â”‚  â”‚   :11434           :6333                                â”‚   â”‚
â”‚  â”‚   (LLM)            (vectors)                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹:**

| Ğ¡ĞµÑ€Ğ²Ğ¸Ñ | Ğ Ğ¾Ğ»ÑŒ Ğ² Context Engineering |
|--------|---------------------------|
| **Ollama** | LLM Ñ tool calling (qwen3:30b-a3b) |
| **LiteLLM** | ĞŸÑ€Ğ¾ĞºÑĞ¸, Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¸Ñ‚ Anthropic API â†’ Ollama |
| **Qdrant** | Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ°Ñ Ğ‘Ğ”, Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ Ñ‡Ğ°Ğ½ĞºĞ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ |
| **MCP Server** | Ğ”Ğ°Ñ‘Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ `search_knowledge` |
| **Claude Code** | ĞÑ€ĞºĞµÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ÑÑ‘, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ |

---

## Ğ§Ğ°ÑÑ‚ÑŒ 2: Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²

### 2.1 Claude Code CLI

```bash
npm install -g @anthropic-ai/claude-code
claude --version
```

### 2.2 Ollama + Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ

```bash
# Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ°
curl -fsSL https://ollama.ai/install.sh | sh

# MoE Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (30B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², 3B Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ…)
ollama pull qwen3:30b-a3b

# Embedding Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
ollama pull mxbai-embed-large
```

**ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ qwen3:30b-a3b:**
- MoE Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°: ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ 30B, ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ 3B
- 18GB VRAM (Ğ²Ğ»ĞµĞ·Ğ°ĞµÑ‚ Ğ² RTX 3090)
- ĞÑ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ tool calling
- Thinking mode Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾

### 2.3 ĞšĞ°ÑÑ‚Ğ¾Ğ¼Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (System Prompt)

```bash
cat > /tmp/Modelfile << 'EOF'
FROM qwen3:30b-a3b

PARAMETER num_ctx 16384
PARAMETER temperature 0.6

SYSTEM """
You are a DevOps assistant with access to tools and a knowledge base.

CRITICAL BEHAVIOR:
1. ALWAYS use search_knowledge FIRST when user asks about concepts
2. ALWAYS use Read tool when user mentions specific files
3. ALWAYS use Bash tool to verify (nginx -t, docker ps, etc.)
4. NEVER hallucinate file contents â€” READ them

RESPONSE PATTERN:
1. Search/Read relevant context
2. Analyze what you found
3. Answer based on ACTUAL data
4. If documentation exists, cite it: "According to nginx-guide.md:45..."

MENTORING STYLE:
- Use Socratic method: ask guiding questions
- Don't give full solutions immediately
- If student is stuck long, give more hints

LANGUAGE: Russian for explanations, English for technical terms.
"""
EOF

ollama create devops-assistant -f /tmp/Modelfile
```

**Ğ­Ñ‚Ğ¾ Context Engineering:** system prompt Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±ÑƒĞ´ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚.

### 2.4 LiteLLM (Ğ¿Ñ€Ğ¾ĞºÑĞ¸)

```yaml
# litellm-config.yaml
model_list:
  - model_name: devops-assistant
    litellm_params:
      model: ollama_chat/devops-assistant:latest
      api_base: http://localhost:11434

general_settings:
  master_key: sk-ant-api03-local-ollama-proxy
```

```bash
docker run -d \
  --name litellm \
  -p 4000:4000 \
  -v $(pwd)/litellm-config.yaml:/app/config.yaml \
  ghcr.io/berriai/litellm:main-latest \
  --config /app/config.yaml
```

### 2.5 Qdrant (Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ°Ñ Ğ‘Ğ”)

```bash
docker run -d \
  --name qdrant \
  -p 6333:6333 \
  -v qdrant_data:/qdrant/storage \
  qdrant/qdrant
```

---

## Ğ§Ğ°ÑÑ‚ÑŒ 3: RAG â€” Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ

### 3.1 ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ñ‡Ğ°Ğ½ĞºĞ¸Ğ½Ğ³Ğ°

```python
# Ğ ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ´Ğ°ĞºÑˆĞµĞ½Ğ°
CHUNK_SIZE = 800       # ~20-30 ÑÑ‚Ñ€Ğ¾Ğº markdown
CHUNK_OVERLAP = 150    # ~15-20% overlap
MAX_EMBED_CHARS = 8000 # Ğ›Ğ¸Ğ¼Ğ¸Ñ‚ embedding Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
VECTOR_SIZE = 1024     # mxbai-embed-large

# Ğ˜Ğ½Ğ´ĞµĞºÑĞ¸Ñ€ÑƒĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ÑƒĞ¶Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸
ALLOWED_DIRS = ["weeks", "bookstack", "application"]
```

**ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ ÑÑ‚Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ:**

```
759 Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² â†’ 16,548 Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²
â‰ˆ 21.8 Ñ‡Ğ°Ğ½ĞºĞ¾Ğ² Ğ½Ğ° Ñ„Ğ°Ğ¹Ğ»
â‰ˆ 800 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² = 20-30 ÑÑ‚Ñ€Ğ¾Ğº markdown = 1 Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±Ğ»Ğ¾Ğº
```

### 3.2 Ğ¤Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ° Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€Ğ° CHUNK_SIZE

```
CHUNK_SIZE = Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ_Ğ´Ğ»Ğ¸Ğ½Ğ°_Ñ„Ğ°Ğ¹Ğ»Ğ° / Ğ–ĞµĞ»Ğ°ĞµĞ¼Ğ¾Ğµ_ĞºĞ¾Ğ»-Ğ²Ğ¾_Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²

ĞĞ°Ñˆ ÑĞ»ÑƒÑ‡Ğ°Ğ¹:
- Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ñ„Ğ°Ğ¹Ğ»: ~17,500 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²
- Ğ¥Ğ¾Ñ‚Ğ¸Ğ¼ ~20 Ñ‡Ğ°Ğ½ĞºĞ¾Ğ² Ğ½Ğ° Ñ„Ğ°Ğ¹Ğ»
- 17,500 / 20 â‰ˆ 875 â†’ Ğ¾ĞºÑ€ÑƒĞ³Ğ»ÑĞµĞ¼ Ğ´Ğ¾ 800
```

**Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°:**

| Ğ¢Ğ¸Ğ¿ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° | CHUNK_SIZE | CHUNK_OVERLAP | ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ |
|--------------|------------|---------------|--------|
| ĞšĞ¾Ğ´ | 500-800 | 100-150 | Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ |
| Markdown docs | 800-1200 | 150-200 | Ğ¡ĞµĞºÑ†Ğ¸Ğ¸ ÑÑ€ĞµĞ´Ğ½Ğ¸Ğµ |
| Ğ”Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ | 1500-2000 | 200-300 | ĞŸĞ°Ñ€Ğ°Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ |

### 3.3 Metadata â€” ĞºĞ»ÑÑ‡ Ğº Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ

```python
def chunk_text(text, file_path):
    """Ğ§Ğ°Ğ½ĞºÑƒĞµĞ¼ Ñ metadata Ğ´Ğ»Ñ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ"""
    chunks = []
    lines = text.split("\n")
    # ...
    chunks.append({
        "content": chunk_text_str,
        "file_path": file_path,      # â† ĞÑ‚ĞºÑƒĞ´Ğ°
        "start_line": start_line,    # â† Ğ“Ğ´Ğµ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾
        "end_line": i - 1            # â† Ğ“Ğ´Ğµ ĞºĞ¾Ğ½ĞµÑ†
    })
    return chunks
```

**Ğ—Ğ°Ñ‡ĞµĞ¼ metadata:**

```
Ğ‘Ğ•Ğ— metadata:
  "Ğ”Ğ»Ñ reverse proxy Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ proxy_pass..."
  â†’ ĞÑ‚ĞºÑƒĞ´Ğ° ÑÑ‚Ğ¾? ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ?

Ğ¡ metadata:
  "Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ nginx-guide.md:45-89, Ğ´Ğ»Ñ reverse proxy..."
  â†’ ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ, Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑŒ Ñ„Ğ°Ğ¹Ğ»
```

### 3.4 Ğ¡ĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸

```python
#!/usr/bin/env python3
"""index_knowledge.py â€” Context Engineering: ÑĞ»Ğ¾Ğ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹"""

import httpx
import hashlib
from pathlib import Path

QDRANT_URL = "http://localhost:6333"
OLLAMA_URL = "http://localhost:11434"
EMBED_MODEL = "mxbai-embed-large"
COLLECTION = "knowledge_base"
CHUNK_SIZE = 800
CHUNK_OVERLAP = 150
MAX_EMBED_CHARS = 8000
ALLOWED_DIRS = ["weeks", "bookstack", "application"]


def get_embedding(text: str) -> list[float]:
    """Embedding Ñ truncation"""
    if len(text) > MAX_EMBED_CHARS:
        text = text[:MAX_EMBED_CHARS]

    response = httpx.post(
        f"{OLLAMA_URL}/api/embeddings",
        json={"model": EMBED_MODEL, "prompt": text},
        timeout=60.0
    )
    return response.json()["embedding"]


def chunk_file(text: str, file_path: str) -> list[dict]:
    """Ğ§Ğ°Ğ½ĞºĞ¸Ğ½Ğ³ Ñ overlap Ğ¸ metadata"""
    chunks = []
    lines = text.split("\n")
    current_chunk = []
    current_size = 0
    start_line = 1

    for i, line in enumerate(lines, 1):
        line_size = len(line) + 1

        if current_size + line_size > CHUNK_SIZE and current_chunk:
            chunk_text = "\n".join(current_chunk)
            if chunk_text.strip():
                chunks.append({
                    "content": chunk_text,
                    "file_path": file_path,
                    "start_line": start_line,
                    "end_line": i - 1
                })

            # Overlap
            overlap_lines = []
            overlap_size = 0
            for ln in reversed(current_chunk):
                if overlap_size + len(ln) > CHUNK_OVERLAP:
                    break
                overlap_lines.insert(0, ln)
                overlap_size += len(ln)

            current_chunk = overlap_lines
            current_size = overlap_size
            start_line = i - len(overlap_lines)

        current_chunk.append(line)
        current_size += line_size

    # ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ Ñ‡Ğ°Ğ½Ğº
    if current_chunk:
        chunk_text = "\n".join(current_chunk)
        if chunk_text.strip():
            chunks.append({
                "content": chunk_text,
                "file_path": file_path,
                "start_line": start_line,
                "end_line": len(lines)
            })

    return chunks


def is_allowed(file_path: Path, base: Path) -> bool:
    """Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹"""
    rel = str(file_path.relative_to(base))
    return any(rel.startswith(d + "/") for d in ALLOWED_DIRS)


def create_collection():
    """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ñ Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ°"""
    httpx.delete(f"{QDRANT_URL}/collections/{COLLECTION}")
    httpx.put(
        f"{QDRANT_URL}/collections/{COLLECTION}",
        json={"vectors": {"size": 1024, "distance": "Cosine"}}
    )


def index_all(docs_path: str):
    """Ğ˜Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ Ñ batch upload"""
    base = Path(docs_path)
    all_files = list(base.rglob("*.md"))
    md_files = [f for f in all_files if is_allowed(f, base)]

    print(f"Total: {len(all_files)}, Filtered: {len(md_files)}")

    all_chunks = []
    for f in md_files:
        text = f.read_text(encoding="utf-8", errors="ignore")
        rel_path = str(f.relative_to(base))
        all_chunks.extend(chunk_file(text, rel_path))

    print(f"Chunks: {len(all_chunks)}")

    # Batch upload
    batch = []
    for chunk in all_chunks:
        emb = get_embedding(chunk["content"])
        batch.append({
            "id": int(hashlib.md5(chunk["content"].encode()).hexdigest()[:15], 16),
            "vector": emb,
            "payload": {
                "document": chunk["content"],
                "metadata": {
                    "file_path": chunk["file_path"],
                    "start_line": chunk["start_line"],
                    "end_line": chunk["end_line"]
                }
            }
        })

        if len(batch) >= 20:
            httpx.put(
                f"{QDRANT_URL}/collections/{COLLECTION}/points",
                json={"points": batch},
                timeout=120.0
            )
            print(f"Uploaded {len(batch)} points")
            batch = []

    if batch:
        httpx.put(
            f"{QDRANT_URL}/collections/{COLLECTION}/points",
            json={"points": batch}
        )


if __name__ == "__main__":
    import sys
    create_collection()
    index_all(sys.argv[1] if len(sys.argv) > 1 else ".")
```

---

## Ğ§Ğ°ÑÑ‚ÑŒ 4: MCP Server â€” Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹

### 4.1 Ğ—Ğ°Ñ‡ĞµĞ¼ ÑĞ²Ğ¾Ğ¹ MCP Server

MCP (Model Context Protocol) â€” ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ´Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. ĞœÑ‹ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ `search_knowledge` â€” Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ğ±Ğ°Ğ·Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹.

**Context Engineering Ğ·Ğ´ĞµÑÑŒ:**
- Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° (JSON Ñ metadata)
- Truncation (Ğ½Ğµ Ğ¿ĞµÑ€ĞµĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚)
- Score (Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´Ğ¸Ñ‚ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ)

### 4.2 Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ

```python
#!/usr/bin/env python3
"""mcp_server.py â€” Context Engineering: ÑĞ»Ğ¾Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²"""

from fastmcp import FastMCP  # Ğ¡Ñ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ÑÑ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ° Ğ¾Ñ‚ Prefect (Ğ½Ğµ Anthropic)
import httpx
import json

QDRANT_URL = "http://localhost:6333"
OLLAMA_URL = "http://localhost:11434"
EMBED_MODEL = "mxbai-embed-large"
COLLECTION = "knowledge_base"

mcp = FastMCP("Knowledge Base")


def get_embedding(text: str) -> list[float]:
    with httpx.Client(timeout=30.0) as client:
        response = client.post(
            f"{OLLAMA_URL}/api/embeddings",
            json={"model": EMBED_MODEL, "prompt": text}
        )
        return response.json()["embedding"]


@mcp.tool()
def search_knowledge(query: str, limit: int = 5) -> str:
    """
    Search the knowledge base for relevant documentation.

    Args:
        query: Natural language search query
        limit: Max results (default: 5)

    Returns:
        JSON array of matching documents with source and relevance score
    """
    embedding = get_embedding(query)

    with httpx.Client(timeout=30.0) as client:
        response = client.post(
            f"{QDRANT_URL}/collections/{COLLECTION}/points/search",
            json={
                "vector": embedding,
                "limit": limit,
                "with_payload": True
            }
        )
        results = response.json().get("result", [])

    # Context Engineering: ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ output
    output = []
    for r in results:
        payload = r.get("payload", {})
        metadata = payload.get("metadata", {})
        output.append({
            "content": payload.get("document", "")[:2000],  # Truncate!
            "source": metadata.get("file_path", "unknown"),
            "lines": f"{metadata.get('start_line', '?')}-{metadata.get('end_line', '?')}",
            "score": round(r.get("score", 0), 3)  # ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´Ğ¸Ñ‚ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ
        })

    return json.dumps(output, ensure_ascii=False, indent=2)


if __name__ == "__main__":
    mcp.run(transport="sse", host="0.0.0.0", port=4002)
```

### 4.3 ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ Ğ²Ğ°Ğ¶ĞµĞ½ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ output

```json
// âŒ ĞŸĞ»Ğ¾Ñ…Ğ¾: Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚
"Ğ”Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ reverse proxy Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ proxy_pass Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¸Ğ²Ñƒ..."

// âœ… Ğ¥Ğ¾Ñ€Ğ¾ÑˆĞ¾: ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° + metadata + score
{
  "content": "## Reverse Proxy\n\nlocation /api/ {\n    proxy_pass http://backend:8000;\n}",
  "source": "nginx-guide.md",
  "lines": "45-89",
  "score": 0.92
}
```

**ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚:**
- Ğ¡Ğ¾ÑĞ»Ğ°Ñ‚ÑŒÑÑ: "Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ nginx-guide.md:45-89..."
- ĞÑ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ: score 0.92 â€” Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ
- ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑŒ Ñ„Ğ°Ğ¹Ğ»: "ĞŸĞ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¸ ÑÑ‚Ñ€Ğ¾ĞºÑƒ 45"

---

## Ğ§Ğ°ÑÑ‚ÑŒ 5: Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ²ÑÑ‘ Ğ²Ğ¼ĞµÑÑ‚Ğµ

### 5.1 Docker Compose

```yaml
version: "3.9"

services:
  ollama:
    image: ollama/ollama
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"

  qdrant:
    image: qdrant/qdrant
    volumes:
      - qdrant_data:/qdrant/storage
    ports:
      - "6333:6333"

  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    command: --config /app/config.yaml
    volumes:
      - ./litellm-config.yaml:/app/config.yaml:ro
    ports:
      - "4000:4000"
    depends_on:
      - ollama

  mcp-rag:
    build: ./mcp-server
    ports:
      - "4002:4002"
    depends_on:
      - qdrant
      - ollama

volumes:
  ollama_data:
  qdrant_data:
```

> âš ï¸ **Ğ’Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ: Security!**
>
> Ğ’ÑĞµ Ğ¿Ğ¾Ñ€Ñ‚Ñ‹ (11434, 6333, 4000, 4002) Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹ **Ğ±ĞµĞ· Ğ°ÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸**.
> Ğ­Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ĞµĞ¼Ğ»ĞµĞ¼Ğ¾ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Ğ½Ğ¾ **ĞĞ• Ğ´Ğ»Ñ production**!
>
> Ğ”Ğ»Ñ production:
> - Qdrant: Ğ²ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ [API key auth](https://qdrant.tech/documentation/guides/security/)
> - LiteLLM: Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ [API keys](https://docs.litellm.ai/docs/proxy/virtual_keys)
> - Firewall: Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ¿Ğ¾Ñ€Ñ‚Ğ°Ğ¼ (iptables/ufw)
> - VPN: Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ WireGuard Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº ÑĞµÑ€Ğ²Ğ¸ÑĞ°Ğ¼

### 5.2 ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ

```bash
# ĞŸĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ
export ANTHROPIC_API_KEY="sk-ant-api03-local-ollama-proxy"
export ANTHROPIC_BASE_URL="http://localhost:4000"

# MCP ÑĞµÑ€Ğ²ĞµÑ€
claude mcp add --transport sse knowledge-base http://localhost:4002/sse

# Ğ—Ğ°Ğ¿ÑƒÑĞº
claude --model devops-assistant
```

### 5.3 ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Context Engineering Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸

```
Ğ’Ñ‹: ĞºĞ°Ğº Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ nginx reverse proxy?

Claude: [search_knowledge("nginx reverse proxy configuration")]

        ĞĞ°ÑˆÑ‘Ğ» Ğ² Ğ±Ğ°Ğ·Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹:

        1. nginx-guide.md:45-89 (score: 0.92)
           "## Reverse Proxy
            location /api/ {
                proxy_pass http://backend:8000;
            }"

        2. troubleshooting.md:12-34 (score: 0.85)
           "### ĞÑˆĞ¸Ğ±ĞºĞ° 502 Bad Gateway..."

        Ğ’Ğ¸Ğ¶Ñƒ, Ñ‡Ñ‚Ğ¾ Ñƒ Ñ‚ĞµĞ±Ñ ĞµÑÑ‚ÑŒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ nginx.
        ĞšĞ°ĞºĞ¾Ğ¹ backend Ñ‚Ñ‹ Ñ…Ğ¾Ñ‡ĞµÑˆÑŒ Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ?
```

**Ğ§Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ¾:**
1. System prompt â†’ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ½Ğ°ĞµÑ‚ Ñ‡Ñ‚Ğ¾ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ¸ÑĞºĞ°Ñ‚ÑŒ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ°
2. search_knowledge â†’ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ»Ğ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ‡Ğ°Ğ½ĞºĞ¸ Ñ metadata
3. Score 0.92 â†’ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ´Ğ¾Ğ²ĞµÑ€ÑÑ‚ÑŒ
4. Source nginx-guide.md:45-89 â†’ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ

---

## Ğ§Ğ°ÑÑ‚ÑŒ 6: ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¸ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ°

### 6.1 Ğ’Ñ€ĞµĞ¼Ñ Ğ¾Ñ‚ĞºĞ»Ğ¸ĞºĞ°

| Ğ­Ñ‚Ğ°Ğ¿ | Ğ’Ñ€ĞµĞ¼Ñ |
|------|-------|
| Embedding Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° | 50-100ms |
| Qdrant search | 10-50ms |
| LLM inference (30B MoE) | 8-15 ÑĞµĞº |
| **ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ»** | **10-20 ÑĞµĞº** |

### 6.2 ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ RAG

```bash
# ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²
curl -s http://localhost:6333/collections/knowledge_base | jq '.result.points_count'
# 16548

# Ğ¢ĞµÑÑ‚ Ğ¿Ğ¾Ğ¸ÑĞºĞ°
curl -X POST http://localhost:6333/collections/knowledge_base/points/search \
  -H "Content-Type: application/json" \
  -d '{"vector": [...], "limit": 3}' | jq '.result[].score'
# 0.92, 0.85, 0.78
```

### 6.3 Ğ¢Ğ¸Ğ¿Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹

| ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° | ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ° | Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ |
|----------|---------|---------|
| ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ¸Ñ‰ĞµÑ‚ Ğ² RAG | System prompt Ğ½Ğµ Ğ½Ğ°ÑÑ‚Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ | Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ "ALWAYS search first" |
| ĞĞ¸Ğ·ĞºĞ¸Ğµ scores (<0.7) | ĞŸĞ»Ğ¾Ñ…Ğ¸Ğµ embeddings | ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ EMBED_MODEL |
| ĞÑ‚Ğ²ĞµÑ‚Ñ‹ Ğ¾Ğ±Ñ€ĞµĞ·Ğ°ÑÑ‚ÑÑ | ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¿ĞµÑ€ĞµĞ¿Ğ¾Ğ»Ğ½ĞµĞ½ | Ğ£Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ limit, truncate |
| Ğ“Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ | ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ tools | ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ tool calling Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ |

---

## Ğ§Ğ°ÑÑ‚ÑŒ 7: Context Engineering Best Practices

### 7.1 Ğ§ĞµĞºĞ»Ğ¸ÑÑ‚

```
[ ] System Prompt Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ (tools first, style)
[ ] RAG Ñ‡Ğ°Ğ½ĞºĞ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° (800-1200 Ğ´Ğ»Ñ docs)
[ ] Metadata Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ (file_path, lines)
[ ] Score Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‘Ñ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ)
[ ] Tool output ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ (JSON, Ğ½Ğµ plain text)
[ ] Truncation Ğ½Ğ° Ğ²ÑĞµÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… (Ğ½Ğµ Ğ¿ĞµÑ€ĞµĞ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚)
[ ] Conversation history Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ° (Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ N ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹)
```

### 7.2 ĞĞ½Ñ‚Ğ¸Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹

```
âŒ "Ğ‘ÑƒĞ´ÑŒ ÑƒĞ¼Ğ½Ñ‹Ğ¼ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ¼" â€” Ğ±ĞµÑĞ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¹ prompt
âœ… "ALWAYS use search_knowledge before answering" â€” ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ

âŒ Ğ§Ğ°Ğ½ĞºĞ¸ Ğ¿Ğ¾ 5000 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² â€” Ğ¿Ğ¾Ñ‚ĞµÑ€Ñ precision
âœ… Ğ§Ğ°Ğ½ĞºĞ¸ Ğ¿Ğ¾ 800 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² â€” Ğ¾Ğ´Ğ¸Ğ½ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±Ğ»Ğ¾Ğº

âŒ Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ‚ÑŒ Ğ²ĞµÑÑŒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ Ğ¸Ğ· RAG â€” Ğ¿ĞµÑ€ĞµĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°
âœ… Truncate Ğ´Ğ¾ 2000 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² + metadata Ğ´Ğ»Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸

âŒ Ğ˜Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ score â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ·Ğ½Ğ°ĞµÑ‚ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ
âœ… ĞŸĞµÑ€ĞµĞ´Ğ°Ğ²Ğ°Ñ‚ÑŒ score â€” "score 0.92 means high confidence"
```

### 7.3 Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°

```
v1: ĞŸÑ€Ğ¾ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚
    "Ğ¢Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ñ‚ Ğ¿Ğ¾ nginx"
    â†’ Ğ“Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸

v2: ĞŸÑ€Ğ¾Ğ¼Ğ¿Ñ‚ + RAG
    System + search_knowledge
    â†’ ĞÑ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸, Ğ½Ğ¾ Ğ±ĞµĞ· Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²

v3: ĞŸÑ€Ğ¾Ğ¼Ğ¿Ñ‚ + RAG + Metadata
    System + search + file_path + lines + score
    â†’ Ğ¦Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸, Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ

v4: ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Context Engineering
    System + RAG + Tools + History + Truncation
    â†’ ĞĞ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğ¹ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸
```

---

## ĞĞ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñ‹ Ğ¿Ğ¾ Ğ¶ĞµĞ»ĞµĞ·Ñƒ

**ĞĞµÑ‚ RTX 4090?** Ğ•ÑÑ‚ÑŒ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹:

| Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ | ĞœĞ¾Ğ´ĞµĞ»ÑŒ | Ğ¡ĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ | Ğ¡Ñ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ |
|---------|--------|----------|-----------|
| **Groq Free Tier** | Llama 3.1 70B | ~200 tok/s | Ğ‘ĞµÑĞ¿Ğ»Ğ°Ñ‚Ğ½Ğ¾ (rate limits) |
| **RTX 3060 12GB** | qwen2.5:14b | ~15 tok/s | Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ |
| **RTX 3080 10GB** | qwen2.5:14b | ~25 tok/s | Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ |
| **CPU (32GB RAM)** | qwen2.5:7b | ~3-5 tok/s | ĞœĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾, Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ |

**Groq â€” Ğ±ĞµÑĞ¿Ğ»Ğ°Ñ‚Ğ½Ğ°Ñ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ°:**

```yaml
# litellm_config.yaml
model_list:
  - model_name: devops-assistant
    litellm_params:
      model: groq/llama-3.1-70b-versatile
      api_key: os.environ/GROQ_API_KEY
```

ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ĞºĞ»ÑÑ‡: [console.groq.com](https://console.groq.com)

**ĞœĞµĞ½ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ»Ğ°Ğ±Ğ¾Ğ³Ğ¾ Ğ¶ĞµĞ»ĞµĞ·Ğ°:**

```bash
# 7B Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ â€” Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° Ğ»ÑĞ±Ğ¾Ğ¼ Ğ¶ĞµĞ»ĞµĞ·Ğµ Ñ 8GB RAM
ollama pull qwen2.5:7b

# 14B Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ â€” Ğ´Ğ»Ñ 12-16GB VRAM
ollama pull qwen2.5:14b-instruct
```

ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ±ÑƒĞ´ĞµÑ‚ Ğ½Ğ¸Ğ¶Ğµ Ñ‡ĞµĞ¼ Ñƒ 30B, Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾.

---

## Ğ˜Ñ‚Ğ¾Ğ³Ğ¾

**Context Engineering > Prompt Engineering**

| ĞÑĞ¿ĞµĞºÑ‚ | Prompt Engineering | Context Engineering |
|--------|-------------------|---------------------|
| Ğ¤Ğ¾ĞºÑƒÑ | Ğ¤Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° | Ğ’ĞµÑÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ |
| Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ | Ğ¢ĞµĞºÑÑ‚ | RAG, Tools, Metadata |
| Ğ’ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ | ĞĞµÑ‚ | Ğ”Ğ° (Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸, scores) |
| Ğ“Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ | Ğ§Ğ°ÑÑ‚Ñ‹Ğµ | Ğ ĞµĞ´ĞºĞ¸Ğµ |
| ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ | ĞĞµÑ‚ | Ğ”Ğ° (Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹) |

**ĞĞ°Ñˆ ÑÑ‚ĞµĞº:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Context Engineering Stack                                       â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  Ollama  â”‚  â”‚ LiteLLM  â”‚  â”‚  Qdrant  â”‚  â”‚   MCP    â”‚        â”‚
â”‚  â”‚  :11434  â”‚â—„â”€â”¤  :4000   â”‚  â”‚  :6333   â”‚â—„â”€â”¤  :4002   â”‚        â”‚
â”‚  â”‚          â”‚  â”‚          â”‚  â”‚          â”‚  â”‚          â”‚        â”‚
â”‚  â”‚ qwen3:   â”‚  â”‚ API      â”‚  â”‚ 16,548   â”‚  â”‚ search_  â”‚        â”‚
â”‚  â”‚ 30b-a3b  â”‚  â”‚ proxy    â”‚  â”‚ chunks   â”‚  â”‚ knowledgeâ”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                  â”‚
â”‚  Context = System + RAG + Tools + History                       â”‚
â”‚  Response = f(Context) â€” Ğ½Ğµ f(Prompt)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Ğ¡ÑÑ‹Ğ»ĞºĞ¸

- [Andrej Karpathy on Context Engineering](https://twitter.com/karpathy)
- [Anthropic: Building Effective Agents](https://www.anthropic.com/research/building-effective-agents)
- [Claude Code Documentation](https://docs.anthropic.com/claude-code)
- [MCP Protocol Specification](https://modelcontextprotocol.io)
- [FastMCP Framework](https://gofastmcp.com)
- [Qdrant Vector Database](https://qdrant.tech)

---

*DevOps Way â€” Context Engineering Ğ½Ğ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ*
