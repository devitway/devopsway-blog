---
title: "Claude Code CLI + RAG + Context Engineering: –ø–æ–ª–Ω—ã–π –≥–∞–π–¥"
date: 2026-02-05T12:00:00+03:00
lastmod: 2026-02-05T12:00:00+03:00
draft: false
weight: 1
categories: ["AI", "DevOps", "Tutorial"]
tags: ["claude-code", "rag", "ollama", "qdrant", "mcp", "context-engineering", "devops", "llm", "vector-database", "litellm"]
author: "DevOps Way"
series: ""
description: "–ö–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å Claude Code CLI —Å –ª–æ–∫–∞–ª—å–Ω—ã–º RAG –∏ –ø–æ—á–µ–º—É context engineering –≤–∞–∂–Ω–µ–µ prompt engineering. –ü–æ–ª–Ω—ã–π —Å—Ç–µ–∫: Ollama, LiteLLM, Qdrant, MCP."
canonical: ""
showToc: true
TocOpen: true
hidemeta: false
comments: true
disableHLJS: false
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: true
ShowRssButtonInSectionTermList: true
UseHugoToc: true
cover:
    image: ""
    alt: "Claude Code + RAG + Context Engineering"
    caption: "–õ–æ–∫–∞–ª—å–Ω—ã–π AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π"
    relative: false
    hidden: false
editPost:
    URL: ""
    Text: "–ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è"
    appendFilePath: true

---

## TL;DR

**Prompt engineering ‚Äî —ç—Ç–æ 5% —É—Å–ø–µ—Ö–∞. Context engineering ‚Äî –æ—Å—Ç–∞–ª—å–Ω—ã–µ 95%.**

–ú—ã –ø–æ—Å—Ç—Ä–æ–∏–º AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π:
- –ó–Ω–∞–µ—Ç –≤–∞—à—É –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é (759 —Ñ–∞–π–ª–æ–≤ ‚Üí 16,548 —á–∞–Ω–∫–æ–≤)
- –í—ã–ø–æ–ª–Ω—è–µ—Ç –∫–æ–º–∞–Ω–¥—ã (tool calling)
- –†–∞–±–æ—Ç–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ (RTX 3090, –Ω–∏–∫–∞–∫–∏—Ö –æ–±–ª–∞–∫–æ–≤)
- –û—Ç–≤–µ—á–∞–µ—Ç –∑–∞ 10-15 —Å–µ–∫—É–Ω–¥

---

## –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Claude Code CLI

```bash
# –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è: Node.js 18+
node --version  # v18.x –∏–ª–∏ –≤—ã—à–µ

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —á–µ—Ä–µ–∑ npm
npm install -g @anthropic-ai/claude-code

# –ò–ª–∏ —á–µ—Ä–µ–∑ npx (–±–µ–∑ —É—Å—Ç–∞–Ω–æ–≤–∫–∏)
npx @anthropic-ai/claude-code

# –ü—Ä–æ–≤–µ—Ä–∫–∞
claude --version
```

**–ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫:**

```bash
# –ó–∞–ø—É—Å–∫ (–ø–æ—Ç—Ä–µ–±—É–µ—Ç API –∫–ª—é—á Anthropic)
claude

# –ò–ª–∏ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏
claude --model claude-sonnet-4-20250514
```

–ü—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ Claude Code –ø–æ–ø—Ä–æ—Å–∏—Ç –≤–≤–µ—Å—Ç–∏ `ANTHROPIC_API_KEY`. –ü–æ–ª—É—á–∏—Ç—å –∫–ª—é—á: [console.anthropic.com](https://console.anthropic.com)

> üí° –í —ç—Ç–æ–º –≥–∞–π–¥–µ –º—ã –Ω–∞—Å—Ç—Ä–æ–∏–º **–ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ LiteLLM**, —á—Ç–æ–±—ã –Ω–µ –ø–ª–∞—Ç–∏—Ç—å –∑–∞ API.

---

## –ß–∞—Å—Ç—å 0: –ß—Ç–æ —Ç–∞–∫–æ–µ Context Engineering

> *"The hottest new programming paradigm is English ‚Äî but the context window is your real IDE"*
>
> *"–°–∞–º–∞—è –≥–æ—Ä—è—á–∞—è –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è ‚Äî —ç—Ç–æ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫. –ù–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ ‚Äî –≤–æ—Ç —Ç–≤–æ—è –Ω–∞—Å—Ç–æ—è—â–∞—è IDE"*
>
> ‚Äî Andrej Karpathy

### Prompt vs Context

{{< mermaid >}}
graph TB
    subgraph PE["‚ùå PROMPT ENGINEERING"]
        P1["–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ nginx.<br/>–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ."]
        P2["–ú–æ–¥–µ–ª—å –≥–∞–ª–ª—é—Ü–∏–Ω–∏—Ä—É–µ—Ç"]
        P3["–ù–µ –∑–Ω–∞–µ—Ç –í–ê–®–£ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é"]
        P4["–ù–µ –º–æ–∂–µ—Ç –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ñ–∞–π–ª—ã"]
        P1 --> P2
        P1 --> P3
        P1 --> P4
    end

    subgraph CE["‚úÖ CONTEXT ENGINEERING"]
        C1["System Prompt"]
        C2["RAG Results"]
        C3["Tool Results"]
        C4["History"]
        C5["–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç"]
        C1 --> C5
        C2 --> C5
        C3 --> C5
        C4 --> C5
    end

    style PE fill:#fee,stroke:#c00
    style CE fill:#efe,stroke:#0a0
{{< /mermaid >}}

### –ê–Ω–∞—Ç–æ–º–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –Ω–∞—à–µ–π —Å–∏—Å—Ç–µ–º–µ

{{< mermaid >}}
flowchart TB
    subgraph CW["CONTEXT WINDOW ~16K tokens"]
        direction TB

        subgraph S1["1. SYSTEM PROMPT ~500 tokens"]
            SP["You are DevOps mentor.<br/>ALWAYS use tools first.<br/>‚Üí –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ü–û–í–ï–î–ï–ù–ò–ï"]
        end

        subgraph S2["2. RAG RESULTS ~2000 tokens"]
            RAG["search_knowledge('nginx')<br/>nginx-guide.md:45-89 (0.92)<br/>‚Üí –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ó–ù–ê–ù–ò–Ø"]
        end

        subgraph S3["3. TOOL RESULTS ~1000 tokens"]
            TR["Read /etc/nginx/nginx.conf<br/>Bash: nginx -t<br/>‚Üí –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –§–ê–ö–¢–´"]
        end

        subgraph S4["4. HISTORY ~2000 tokens"]
            H["User: nginx –Ω–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è<br/>Assistant: –î–∞–≤–∞–π –ø–æ—Å–º–æ—Ç—Ä–∏–º...<br/>‚Üí –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ö–û–ù–¢–ï–ö–°–¢"]
        end

        subgraph S5["5. LLM RESPONSE"]
            R["–í–∏–∂—É –≤ nginx.conf:45 –æ—à–∏–±–∫–∞.<br/>–°–æ–≥–ª–∞—Å–Ω–æ nginx-guide.md..."]
        end

        S1 --> S5
        S2 --> S5
        S3 --> S5
        S4 --> S5
    end

    style S1 fill:#e3f2fd
    style S2 fill:#e8f5e9
    style S3 fill:#fff3e0
    style S4 fill:#fce4ec
    style S5 fill:#f3e5f5
{{< /mermaid >}}

### –ì–¥–µ –º—ã –∏–Ω–∂–µ–Ω–µ—Ä–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç

| –°–ª–æ–π | –ß—Ç–æ –¥–µ–ª–∞–µ–º | –ó–∞—á–µ–º |
|------|------------|-------|
| **Chunking** | `CHUNK_SIZE=800`, overlap=150 | –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä "–ø–æ—Ä—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π" |
| **Metadata** | file_path, start_line, end_line | –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç —Å–æ—Å–ª–∞—Ç—å—Å—è –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫ |
| **Retrieval** | top-5 –ø–æ cosine similarity | –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∑–Ω–∞–Ω–∏—è, –Ω–µ —à—É–º |
| **Score** | –ü–µ—Ä–µ–¥–∞—ë–º score –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö | –ú–æ–¥–µ–ª—å –≤–∏–¥–∏—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å |
| **System Prompt** | Tools first, Socratic method | –ü–æ–≤–µ–¥–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ |
| **Tool Design** | JSON output, truncation | –ü—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç |

---

## –ß–∞—Å—Ç—å 1: –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

{{< mermaid >}}
flowchart TB
    T["üñ•Ô∏è –í–∞—à —Ç–µ—Ä–º–∏–Ω–∞–ª<br/>claude '–≤–æ–ø—Ä–æ—Å'"]

    subgraph CC["Claude Code CLI"]
        direction TB
        L["LiteLLM<br/>:4000"]
        M["MCP Server<br/>:4002"]
        B["Built-in tools<br/>Bash, Read..."]

        O["Ollama<br/>:11434<br/>LLM"]
        Q["Qdrant<br/>:6333<br/>vectors"]

        L --> O
        M --> Q
        M --> O
    end

    T --> CC

    style T fill:#e3f2fd
    style O fill:#fff3e0
    style Q fill:#e8f5e9
    style L fill:#fce4ec
    style M fill:#f3e5f5
{{< /mermaid >}}

**–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:**

| –°–µ—Ä–≤–∏—Å | –†–æ–ª—å –≤ Context Engineering |
|--------|---------------------------|
| **Ollama** | LLM —Å tool calling (qwen3:30b-a3b) |
| **LiteLLM** | –ü—Ä–æ–∫—Å–∏, –ø–µ—Ä–µ–≤–æ–¥–∏—Ç Anthropic API ‚Üí Ollama |
| **Qdrant** | –í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î, —Ö—Ä–∞–Ω–∏—Ç —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ |
| **MCP Server** | –î–∞—ë—Ç –º–æ–¥–µ–ª–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç `search_knowledge` |
| **Claude Code** | –û—Ä–∫–µ—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—Å—ë, —É–ø—Ä–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º |

---

## –ß–∞—Å—Ç—å 2: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤

### 2.1 Claude Code CLI

```bash
npm install -g @anthropic-ai/claude-code
claude --version
```

### 2.2 Ollama + –º–æ–¥–µ–ª—å

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞
curl -fsSL https://ollama.ai/install.sh | sh

# MoE –º–æ–¥–µ–ª—å (30B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, 3B –∞–∫—Ç–∏–≤–Ω—ã—Ö)
ollama pull qwen3:30b-a3b

# Embedding –º–æ–¥–µ–ª—å
ollama pull mxbai-embed-large
```

**–ü–æ—á–µ–º—É qwen3:30b-a3b:**
- MoE –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: –∫–∞—á–µ—Å—Ç–≤–æ 30B, —Å–∫–æ—Ä–æ—Å—Ç—å 3B
- 18GB VRAM (–≤–ª–µ–∑–∞–µ—Ç –≤ RTX 3090)
- –û—Ç–ª–∏—á–Ω—ã–π tool calling
- Thinking mode —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ

### 2.3 –ö–∞—Å—Ç–æ–º–Ω–∞—è –º–æ–¥–µ–ª—å (System Prompt)

```bash
cat > /tmp/Modelfile << 'EOF'
FROM qwen3:30b-a3b

PARAMETER num_ctx 16384
PARAMETER temperature 0.6

SYSTEM """
You are a DevOps assistant with access to tools and a knowledge base.

CRITICAL BEHAVIOR:
1. ALWAYS use search_knowledge FIRST when user asks about concepts
2. ALWAYS use Read tool when user mentions specific files
3. ALWAYS use Bash tool to verify (nginx -t, docker ps, etc.)
4. NEVER hallucinate file contents ‚Äî READ them

RESPONSE PATTERN:
1. Search/Read relevant context
2. Analyze what you found
3. Answer based on ACTUAL data
4. If documentation exists, cite it: "According to nginx-guide.md:45..."

MENTORING STYLE:
- Use Socratic method: ask guiding questions
- Don't give full solutions immediately
- If student is stuck long, give more hints

LANGUAGE: Russian for explanations, English for technical terms.
"""
EOF

ollama create devops-assistant -f /tmp/Modelfile
```

**–≠—Ç–æ Context Engineering:** system prompt –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞–∫ –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.

### 2.4 LiteLLM (–ø—Ä–æ–∫—Å–∏)

```yaml
# litellm-config.yaml
model_list:
  - model_name: devops-assistant
    litellm_params:
      model: ollama_chat/devops-assistant:latest
      api_base: http://localhost:11434

general_settings:
  master_key: sk-ant-api03-local-ollama-proxy
```

```bash
docker run -d \
  --name litellm \
  -p 4000:4000 \
  -v $(pwd)/litellm-config.yaml:/app/config.yaml \
  ghcr.io/berriai/litellm:main-latest \
  --config /app/config.yaml
```

### 2.5 Qdrant (–≤–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î)

```bash
docker run -d \
  --name qdrant \
  -p 6333:6333 \
  -v qdrant_data:/qdrant/storage \
  qdrant/qdrant
```

---

## –ß–∞—Å—Ç—å 3: RAG ‚Äî –∏–Ω–∂–µ–Ω–µ—Ä–∏–º –∑–Ω–∞–Ω–∏—è

### 3.1 –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —á–∞–Ω–∫–∏–Ω–≥–∞

```python
# –†–µ–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞
CHUNK_SIZE = 800       # ~20-30 —Å—Ç—Ä–æ–∫ markdown
CHUNK_OVERLAP = 150    # ~15-20% overlap
MAX_EMBED_CHARS = 8000 # –õ–∏–º–∏—Ç embedding –º–æ–¥–µ–ª–∏
VECTOR_SIZE = 1024     # mxbai-embed-large

# –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
ALLOWED_DIRS = ["docs", "guides", "tutorials"]
```

**–ü–æ—á–µ–º—É —ç—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏—è:**

```
759 —Ñ–∞–π–ª–æ–≤ ‚Üí 16,548 —á–∞–Ω–∫–æ–≤
‚âà 21.8 —á–∞–Ω–∫–æ–≤ –Ω–∞ —Ñ–∞–π–ª
‚âà 800 —Å–∏–º–≤–æ–ª–æ–≤ = 20-30 —Å—Ç—Ä–æ–∫ markdown = 1 –ª–æ–≥–∏—á–µ—Å–∫–∏–π –±–ª–æ–∫
```

### 3.2 –§–æ—Ä–º—É–ª–∞ –ø–æ–¥–±–æ—Ä–∞ CHUNK_SIZE

```
CHUNK_SIZE = –°—Ä–µ–¥–Ω—è—è_–¥–ª–∏–Ω–∞_—Ñ–∞–π–ª–∞ / –ñ–µ–ª–∞–µ–º–æ–µ_–∫–æ–ª-–≤–æ_—á–∞–Ω–∫–æ–≤

–ù–∞—à —Å–ª—É—á–∞–π:
- –°—Ä–µ–¥–Ω–∏–π —Ñ–∞–π–ª: ~17,500 —Å–∏–º–≤–æ–ª–æ–≤
- –•–æ—Ç–∏–º ~20 —á–∞–Ω–∫–æ–≤ –Ω–∞ —Ñ–∞–π–ª
- 17,500 / 20 ‚âà 875 ‚Üí –æ–∫—Ä—É–≥–ª—è–µ–º –¥–æ 800
```

**–≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞:**

| –¢–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞ | CHUNK_SIZE | CHUNK_OVERLAP | –ü–æ—á–µ–º—É |
|--------------|------------|---------------|--------|
| –ö–æ–¥ | 500-800 | 100-150 | –§—É–Ω–∫—Ü–∏–∏ –∫–æ—Ä–æ—Ç–∫–∏–µ |
| Markdown docs | 800-1200 | 150-200 | –°–µ–∫—Ü–∏–∏ —Å—Ä–µ–¥–Ω–∏–µ |
| –î–ª–∏–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ | 1500-2000 | 200-300 | –ü–∞—Ä–∞–≥—Ä–∞—Ñ—ã –±–æ–ª—å—à–∏–µ |

### 3.3 Metadata ‚Äî –∫–ª—é—á –∫ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—é

```python
def chunk_text(text, file_path):
    """–ß–∞–Ω–∫—É–µ–º —Å metadata –¥–ª—è —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    chunks = []
    lines = text.split("\n")
    # ...
    chunks.append({
        "content": chunk_text_str,
        "file_path": file_path,      # ‚Üê –û—Ç–∫—É–¥–∞
        "start_line": start_line,    # ‚Üê –ì–¥–µ –Ω–∞—á–∞–ª–æ
        "end_line": i - 1            # ‚Üê –ì–¥–µ –∫–æ–Ω–µ—Ü
    })
    return chunks
```

**–ó–∞—á–µ–º metadata:**

```
–ë–ï–ó metadata:
  "–î–ª—è reverse proxy –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ proxy_pass..."
  ‚Üí –û—Ç–∫—É–¥–∞ —ç—Ç–æ? –ú–æ–∂–Ω–æ –≤–µ—Ä–∏—Ç—å?

–° metadata:
  "–°–æ–≥–ª–∞—Å–Ω–æ nginx-guide.md:45-89, –¥–ª—è reverse proxy..."
  ‚Üí –ú–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –º–æ–∂–Ω–æ –æ—Ç–∫—Ä—ã—Ç—å —Ñ–∞–π–ª
```

### 3.4 –°–∫—Ä–∏–ø—Ç –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏

```python
#!/usr/bin/env python3
"""index_knowledge.py ‚Äî Context Engineering: —Å–ª–æ–π –∑–Ω–∞–Ω–∏–π"""

import httpx
import hashlib
from pathlib import Path

QDRANT_URL = "http://localhost:6333"
OLLAMA_URL = "http://localhost:11434"
EMBED_MODEL = "mxbai-embed-large"
COLLECTION = "knowledge_base"
CHUNK_SIZE = 800
CHUNK_OVERLAP = 150
MAX_EMBED_CHARS = 8000
ALLOWED_DIRS = ["docs", "guides", "tutorials"]


def get_embedding(text: str) -> list[float]:
    """Embedding —Å truncation"""
    if len(text) > MAX_EMBED_CHARS:
        text = text[:MAX_EMBED_CHARS]

    response = httpx.post(
        f"{OLLAMA_URL}/api/embeddings",
        json={"model": EMBED_MODEL, "prompt": text},
        timeout=60.0
    )
    return response.json()["embedding"]


def chunk_file(text: str, file_path: str) -> list[dict]:
    """–ß–∞–Ω–∫–∏–Ω–≥ —Å overlap –∏ metadata"""
    chunks = []
    lines = text.split("\n")
    current_chunk = []
    current_size = 0
    start_line = 1

    for i, line in enumerate(lines, 1):
        line_size = len(line) + 1

        if current_size + line_size > CHUNK_SIZE and current_chunk:
            chunk_text = "\n".join(current_chunk)
            if chunk_text.strip():
                chunks.append({
                    "content": chunk_text,
                    "file_path": file_path,
                    "start_line": start_line,
                    "end_line": i - 1
                })

            # Overlap
            overlap_lines = []
            overlap_size = 0
            for ln in reversed(current_chunk):
                if overlap_size + len(ln) > CHUNK_OVERLAP:
                    break
                overlap_lines.insert(0, ln)
                overlap_size += len(ln)

            current_chunk = overlap_lines
            current_size = overlap_size
            start_line = i - len(overlap_lines)

        current_chunk.append(line)
        current_size += line_size

    # –ü–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
    if current_chunk:
        chunk_text = "\n".join(current_chunk)
        if chunk_text.strip():
            chunks.append({
                "content": chunk_text,
                "file_path": file_path,
                "start_line": start_line,
                "end_line": len(lines)
            })

    return chunks


def is_allowed(file_path: Path, base: Path) -> bool:
    """–§–∏–ª—å—Ç—Ä –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
    rel = str(file_path.relative_to(base))
    return any(rel.startswith(d + "/") for d in ALLOWED_DIRS)


def create_collection():
    """–°–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º –≤–µ–∫—Ç–æ—Ä–∞"""
    httpx.delete(f"{QDRANT_URL}/collections/{COLLECTION}")
    httpx.put(
        f"{QDRANT_URL}/collections/{COLLECTION}",
        json={"vectors": {"size": 1024, "distance": "Cosine"}}
    )


def index_all(docs_path: str):
    """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å batch upload"""
    base = Path(docs_path)
    all_files = list(base.rglob("*.md"))
    md_files = [f for f in all_files if is_allowed(f, base)]

    print(f"Total: {len(all_files)}, Filtered: {len(md_files)}")

    all_chunks = []
    for f in md_files:
        text = f.read_text(encoding="utf-8", errors="ignore")
        rel_path = str(f.relative_to(base))
        all_chunks.extend(chunk_file(text, rel_path))

    print(f"Chunks: {len(all_chunks)}")

    # Batch upload
    batch = []
    for chunk in all_chunks:
        emb = get_embedding(chunk["content"])
        batch.append({
            "id": int(hashlib.md5(chunk["content"].encode()).hexdigest()[:15], 16),
            "vector": emb,
            "payload": {
                "document": chunk["content"],
                "metadata": {
                    "file_path": chunk["file_path"],
                    "start_line": chunk["start_line"],
                    "end_line": chunk["end_line"]
                }
            }
        })

        if len(batch) >= 20:
            httpx.put(
                f"{QDRANT_URL}/collections/{COLLECTION}/points",
                json={"points": batch},
                timeout=120.0
            )
            print(f"Uploaded {len(batch)} points")
            batch = []

    if batch:
        httpx.put(
            f"{QDRANT_URL}/collections/{COLLECTION}/points",
            json={"points": batch}
        )


if __name__ == "__main__":
    import sys
    create_collection()
    index_all(sys.argv[1] if len(sys.argv) > 1 else ".")
```

---

## –ß–∞—Å—Ç—å 4: MCP Server ‚Äî –∏–Ω–∂–µ–Ω–µ—Ä–∏–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã

### 4.1 –ó–∞—á–µ–º —Å–≤–æ–π MCP Server

MCP (Model Context Protocol) ‚Äî —Å–ø–æ—Å–æ–± –¥–∞—Ç—å –º–æ–¥–µ–ª–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã. –ú—ã —Å–æ–∑–¥–∞—ë–º `search_knowledge` ‚Äî –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –ø–æ–∏—Å–∫–∞ –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.

**Context Engineering –∑–¥–µ—Å—å:**
- –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ (JSON —Å metadata)
- Truncation (–Ω–µ –ø–µ—Ä–µ–ø–æ–ª–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç)
- Score (–º–æ–¥–µ–ª—å –≤–∏–¥–∏—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)

### 4.2 –†–µ–∞–ª–∏–∑–∞—Ü–∏—è

```python
#!/usr/bin/env python3
"""mcp_server.py ‚Äî Context Engineering: —Å–ª–æ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤"""

from fastmcp import FastMCP  # –°—Ç–æ—Ä–æ–Ω–Ω—è—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –æ—Ç Prefect (–Ω–µ Anthropic)
import httpx
import json

QDRANT_URL = "http://localhost:6333"
OLLAMA_URL = "http://localhost:11434"
EMBED_MODEL = "mxbai-embed-large"
COLLECTION = "knowledge_base"

mcp = FastMCP("Knowledge Base")


def get_embedding(text: str) -> list[float]:
    with httpx.Client(timeout=30.0) as client:
        response = client.post(
            f"{OLLAMA_URL}/api/embeddings",
            json={"model": EMBED_MODEL, "prompt": text}
        )
        return response.json()["embedding"]


@mcp.tool()
def search_knowledge(query: str, limit: int = 5) -> str:
    """
    Search the knowledge base for relevant documentation.

    Args:
        query: Natural language search query
        limit: Max results (default: 5)

    Returns:
        JSON array of matching documents with source and relevance score
    """
    embedding = get_embedding(query)

    with httpx.Client(timeout=30.0) as client:
        response = client.post(
            f"{QDRANT_URL}/collections/{COLLECTION}/points/search",
            json={
                "vector": embedding,
                "limit": limit,
                "with_payload": True
            }
        )
        results = response.json().get("result", [])

    # Context Engineering: —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π output
    output = []
    for r in results:
        payload = r.get("payload", {})
        metadata = payload.get("metadata", {})
        output.append({
            "content": payload.get("document", "")[:2000],  # Truncate!
            "source": metadata.get("file_path", "unknown"),
            "lines": f"{metadata.get('start_line', '?')}-{metadata.get('end_line', '?')}",
            "score": round(r.get("score", 0), 3)  # –ú–æ–¥–µ–ª—å –≤–∏–¥–∏—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        })

    return json.dumps(output, ensure_ascii=False, indent=2)


if __name__ == "__main__":
    mcp.run(transport="sse", host="0.0.0.0", port=4002)
```

### 4.3 –ü–æ—á–µ–º—É –≤–∞–∂–µ–Ω —Ñ–æ—Ä–º–∞—Ç output

```json
// ‚ùå –ü–ª–æ—Ö–æ: –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç
"–î–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ reverse proxy –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ proxy_pass –¥–∏—Ä–µ–∫—Ç–∏–≤—É..."

// ‚úÖ –•–æ—Ä–æ—à–æ: —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ + metadata + score
{
  "content": "## Reverse Proxy\n\nlocation /api/ {\n    proxy_pass http://backend:8000;\n}",
  "source": "nginx-guide.md",
  "lines": "45-89",
  "score": 0.92
}
```

**–ú–æ–¥–µ–ª—å —Ç–µ–ø–µ—Ä—å –º–æ–∂–µ—Ç:**
- –°–æ—Å–ª–∞—Ç—å—Å—è: "–°–æ–≥–ª–∞—Å–Ω–æ nginx-guide.md:45-89..."
- –û—Ü–µ–Ω–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: score 0.92 ‚Äî –≤—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
- –ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å –æ—Ç–∫—Ä—ã—Ç—å —Ñ–∞–π–ª: "–ü–æ—Å–º–æ—Ç—Ä–∏ —Å—Ç—Ä–æ–∫—É 45"

---

## –ß–∞—Å—Ç—å 5: –°–æ–±–∏—Ä–∞–µ–º –≤—Å—ë –≤–º–µ—Å—Ç–µ

### 5.1 Docker Compose

```yaml
version: "3.9"

services:
  ollama:
    image: ollama/ollama
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"

  qdrant:
    image: qdrant/qdrant
    volumes:
      - qdrant_data:/qdrant/storage
    ports:
      - "6333:6333"

  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    command: --config /app/config.yaml
    volumes:
      - ./litellm-config.yaml:/app/config.yaml:ro
    ports:
      - "4000:4000"
    depends_on:
      - ollama

  mcp-rag:
    build: ./mcp-server
    ports:
      - "4002:4002"
    depends_on:
      - qdrant
      - ollama

volumes:
  ollama_data:
  qdrant_data:
```

> ‚ö†Ô∏è **–í–Ω–∏–º–∞–Ω–∏–µ: Security!**
>
> –í—Å–µ –ø–æ—Ä—Ç—ã (11434, 6333, 4000, 4002) –æ—Ç–∫—Ä—ã—Ç—ã **–±–µ–∑ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏**.
> –≠—Ç–æ –ø—Ä–∏–µ–º–ª–µ–º–æ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏, –Ω–æ **–ù–ï –¥–ª—è production**!
>
> –î–ª—è production:
> - Qdrant: –≤–∫–ª—é—á–∏—Ç—å [API key auth](https://qdrant.tech/documentation/guides/security/)
> - LiteLLM: –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å [API keys](https://docs.litellm.ai/docs/proxy/virtual_keys)
> - Firewall: –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ –ø–æ—Ä—Ç–∞–º (iptables/ufw)
> - VPN: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å WireGuard –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ —Å–µ—Ä–≤–∏—Å–∞–º

### 5.2 –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ

```bash
# –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
export ANTHROPIC_API_KEY="sk-ant-api03-local-ollama-proxy"
export ANTHROPIC_BASE_URL="http://localhost:4000"

# MCP —Å–µ—Ä–≤–µ—Ä
claude mcp add --transport sse knowledge-base http://localhost:4002/sse

# –ó–∞–ø—É—Å–∫
claude --model devops-assistant
```

### 5.3 –ü—Ä–æ–≤–µ—Ä–∫–∞ Context Engineering –≤ –¥–µ–π—Å—Ç–≤–∏–∏

```
–í—ã: –∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å nginx reverse proxy?

Claude: [search_knowledge("nginx reverse proxy configuration")]

        –ù–∞—à—ë–ª –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π:

        1. nginx-guide.md:45-89 (score: 0.92)
           "## Reverse Proxy
            location /api/ {
                proxy_pass http://backend:8000;
            }"

        2. troubleshooting.md:12-34 (score: 0.85)
           "### –û—à–∏–±–∫–∞ 502 Bad Gateway..."

        –í–∏–∂—É, —á—Ç–æ —É —Ç–µ–±—è –µ—Å—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ nginx.
        –ö–∞–∫–æ–π backend —Ç—ã —Ö–æ—á–µ—à—å –ø—Ä–æ–∫—Å–∏—Ä–æ–≤–∞—Ç—å?
```

**–ß—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ:**
1. System prompt ‚Üí –º–æ–¥–µ–ª—å –∑–Ω–∞–µ—Ç —á—Ç–æ –Ω—É–∂–Ω–æ –∏—Å–∫–∞—Ç—å —Å–Ω–∞—á–∞–ª–∞
2. search_knowledge ‚Üí –ø–æ–ª—É—á–∏–ª–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ —Å metadata
3. Score 0.92 ‚Üí –≤—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å, –º–æ–∂–Ω–æ –¥–æ–≤–µ—Ä—è—Ç—å
4. Source nginx-guide.md:45-89 ‚Üí –º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å

---

## –ß–∞—Å—Ç—å 6: –ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ç–ª–∞–¥–∫–∞

### 6.1 –í—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞

| –≠—Ç–∞–ø | –í—Ä–µ–º—è |
|------|-------|
| Embedding –∑–∞–ø—Ä–æ—Å–∞ | 50-100ms |
| Qdrant search | 10-50ms |
| LLM inference (30B MoE) | 8-15 —Å–µ–∫ |
| **–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª** | **10-20 —Å–µ–∫** |

### 6.2 –ö–∞—á–µ—Å—Ç–≤–æ RAG

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤
curl -s http://localhost:6333/collections/knowledge_base | jq '.result.points_count'
# 16548

# –¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞
curl -X POST http://localhost:6333/collections/knowledge_base/points/search \
  -H "Content-Type: application/json" \
  -d '{"vector": [...], "limit": 3}' | jq '.result[].score'
# 0.92, 0.85, 0.78
```

### 6.3 –¢–∏–ø–∏—á–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã

| –ü—Ä–æ–±–ª–µ–º–∞ | –ü—Ä–∏—á–∏–Ω–∞ | –†–µ—à–µ–Ω–∏–µ |
|----------|---------|---------|
| –ú–æ–¥–µ–ª—å –Ω–µ –∏—â–µ—Ç –≤ RAG | System prompt –Ω–µ –Ω–∞—Å—Ç–∞–∏–≤–∞–µ—Ç | –î–æ–±–∞–≤–∏—Ç—å "ALWAYS search first" |
| –ù–∏–∑–∫–∏–µ scores (<0.7) | –ü–ª–æ—Ö–∏–µ embeddings | –ü—Ä–æ–≤–µ—Ä–∏—Ç—å EMBED_MODEL |
| –û—Ç–≤–µ—Ç—ã –æ–±—Ä–µ–∑–∞—é—Ç—Å—è | –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω | –£–º–µ–Ω—å—à–∏—Ç—å limit, truncate |
| –ì–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ | –ú–æ–¥–µ–ª—å –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç tools | –ü—Ä–æ–≤–µ—Ä–∏—Ç—å tool calling –º–æ–¥–µ–ª–∏ |

---

## –ß–∞—Å—Ç—å 7: Context Engineering Best Practices

### 7.1 –ß–µ–∫–ª–∏—Å—Ç

```
[ ] System Prompt –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ (tools first, style)
[ ] RAG —á–∞–Ω–∫–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ (800-1200 –¥–ª—è docs)
[ ] Metadata –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ü–∏—Ç–∏—Ä–æ–≤–∞—Ç—å (file_path, lines)
[ ] Score –ø–µ—Ä–µ–¥–∞—ë—Ç—Å—è –º–æ–¥–µ–ª–∏ (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)
[ ] Tool output —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω (JSON, –Ω–µ plain text)
[ ] Truncation –Ω–∞ –≤—Å–µ—Ö —É—Ä–æ–≤–Ω—è—Ö (–Ω–µ –ø–µ—Ä–µ–ø–æ–ª–Ω—è—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç)
[ ] Conversation history –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ N —Å–æ–æ–±—â–µ–Ω–∏–π)
```

### 7.2 –ê–Ω—Ç–∏–ø–∞—Ç—Ç–µ—Ä–Ω—ã

```
‚ùå "–ë—É–¥—å —É–º–Ω—ã–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º" ‚Äî –±–µ—Å–ø–æ–ª–µ–∑–Ω—ã–π prompt
‚úÖ "ALWAYS use search_knowledge before answering" ‚Äî –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ

‚ùå –ß–∞–Ω–∫–∏ –ø–æ 5000 —Å–∏–º–≤–æ–ª–æ–≤ ‚Äî –ø–æ—Ç–µ—Ä—è precision
‚úÖ –ß–∞–Ω–∫–∏ –ø–æ 800 —Å–∏–º–≤–æ–ª–æ–≤ ‚Äî –æ–¥–∏–Ω –ª–æ–≥–∏—á–µ—Å–∫–∏–π –±–ª–æ–∫

‚ùå –í–æ–∑–≤—Ä–∞—â–∞—Ç—å –≤–µ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç –∏–∑ RAG ‚Äî –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
‚úÖ Truncate –¥–æ 2000 —Å–∏–º–≤–æ–ª–æ–≤ + metadata –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏

‚ùå –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å score ‚Äî –º–æ–¥–µ–ª—å –Ω–µ –∑–Ω–∞–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
‚úÖ –ü–µ—Ä–µ–¥–∞–≤–∞—Ç—å score ‚Äî "score 0.92 means high confidence"
```

### 7.3 –≠–≤–æ–ª—é—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

```
v1: –ü—Ä–æ—Å—Ç–æ –ø—Ä–æ–º–ø—Ç
    "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ nginx"
    ‚Üí –ì–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏

v2: –ü—Ä–æ–º–ø—Ç + RAG
    System + search_knowledge
    ‚Üí –û—Ç–≤–µ—á–∞–µ—Ç –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏, –Ω–æ –±–µ–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤

v3: –ü—Ä–æ–º–ø—Ç + RAG + Metadata
    System + search + file_path + lines + score
    ‚Üí –¶–∏—Ç–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∏, –º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å

v4: –ü–æ–ª–Ω—ã–π Context Engineering
    System + RAG + Tools + History + Truncation
    ‚Üí –ù–∞–¥—ë–∂–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏
```

---

## –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã –ø–æ –∂–µ–ª–µ–∑—É

**–ù–µ—Ç RTX 3090?** –ï—Å—Ç—å –≤–∞—Ä–∏–∞–Ω—Ç—ã:

| –í–∞—Ä–∏–∞–Ω—Ç | –ú–æ–¥–µ–ª—å | –°–∫–æ—Ä–æ—Å—Ç—å | –°—Ç–æ–∏–º–æ—Å—Ç—å |
|---------|--------|----------|-----------|
| **Groq Free Tier** | Llama 3.1 70B | ~200 tok/s | –ë–µ—Å–ø–ª–∞—Ç–Ω–æ (rate limits) |
| **RTX 3060 12GB** | qwen2.5:14b | ~15 tok/s | –†–∞–±–æ—Ç–∞–µ—Ç |
| **RTX 3080 10GB** | qwen2.5:14b | ~25 tok/s | –†–∞–±–æ—Ç–∞–µ—Ç |
| **CPU (32GB RAM)** | qwen2.5:7b | ~3-5 tok/s | –ú–µ–¥–ª–µ–Ω–Ω–æ, –Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç |

**Groq ‚Äî –±–µ—Å–ø–ª–∞—Ç–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞:**

```yaml
# litellm_config.yaml
model_list:
  - model_name: devops-assistant
    litellm_params:
      model: groq/llama-3.1-70b-versatile
      api_key: os.environ/GROQ_API_KEY
```

–ü–æ–ª—É—á–∏—Ç—å –∫–ª—é—á: [console.groq.com](https://console.groq.com)

**–ú–µ–Ω—å—à–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–ª–∞–±–æ–≥–æ –∂–µ–ª–µ–∑–∞:**

```bash
# 7B –º–æ–¥–µ–ª—å ‚Äî —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –ª—é–±–æ–º –∂–µ–ª–µ–∑–µ —Å 8GB RAM
ollama pull qwen2.5:7b

# 14B –º–æ–¥–µ–ª—å ‚Äî –¥–ª—è 12-16GB VRAM
ollama pull qwen2.5:14b-instruct
```

–ö–∞—á–µ—Å—Ç–≤–æ –±—É–¥–µ—Ç –Ω–∏–∂–µ —á–µ–º —É 30B, –Ω–æ –¥–ª—è –º–Ω–æ–≥–∏—Ö –∑–∞–¥–∞—á –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ.

---

## –ò—Ç–æ–≥–æ

**Context Engineering > Prompt Engineering**

| –ê—Å–ø–µ–∫—Ç | Prompt Engineering | Context Engineering |
|--------|-------------------|---------------------|
| –§–æ–∫—É—Å | –§–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ | –í–µ—Å—å –∫–æ–Ω—Ç–µ–∫—Å—Ç |
| –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã | –¢–µ–∫—Å—Ç | RAG, Tools, Metadata |
| –í–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º–æ—Å—Ç—å | –ù–µ—Ç | –î–∞ (–∏—Å—Ç–æ—á–Ω–∏–∫–∏, scores) |
| –ì–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ | –ß–∞—Å—Ç—ã–µ | –†–µ–¥–∫–∏–µ |
| –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å | –ù–µ—Ç | –î–∞ (–¥–æ–±–∞–≤–ª—è–π –¥–æ–∫—É–º–µ–Ω—Ç—ã) |

**–ù–∞—à —Å—Ç–µ–∫:**

{{< mermaid >}}
flowchart LR
    subgraph Stack["Context Engineering Stack"]
        direction LR
        O["ü¶ô Ollama<br/>:11434<br/>qwen3:30b"]
        L["üîÄ LiteLLM<br/>:4000<br/>API proxy"]
        Q["üîç Qdrant<br/>:6333<br/>16,548 chunks"]
        M["üîß MCP<br/>:4002<br/>search_knowledge"]

        L --> O
        M --> Q
        M --> O
    end

    F["Context = System + RAG + Tools + History<br/>Response = f(Context)"]

    Stack --> F

    style O fill:#fff3e0
    style L fill:#fce4ec
    style Q fill:#e8f5e9
    style M fill:#e3f2fd
    style F fill:#f5f5f5
{{< /mermaid >}}

---

## –°—Å—ã–ª–∫–∏

- [Andrej Karpathy on Context Engineering](https://twitter.com/karpathy)
- [Anthropic: Building Effective Agents](https://www.anthropic.com/research/building-effective-agents)
- [Claude Code Documentation](https://docs.anthropic.com/claude-code)
- [MCP Protocol Specification](https://modelcontextprotocol.io)
- [FastMCP Framework](https://gofastmcp.com)
- [Qdrant Vector Database](https://qdrant.tech)

---

*DevOps Way ‚Äî Context Engineering –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ*
